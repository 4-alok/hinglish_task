The tutorial was quite helpful in understanding the concept.
Tutorial ne concept samajhne mein kaafi madad ki.

I'm going to share some quick tips for better photography.
Main kuch tezi se photography ke behtar tips share karne ja raha hoon.

Don't forget to turn on the notification bell to stay updated.
Notification bell ko on kar lena mat bhoolna taaki updated reh sakein.

I was really excited about the new gadget launch.
Naye gadget launch ke liye main sach mein excited tha.

The event was a huge success with a massive turnout.
Event bada successful tha, bahut saare log aaye the.

Explaining complex topics in a simple way is my goal.
Complex topics ko simple tareeke se samjhana mera goal hai.

It took me a while to figure out the settings.
Settings ko samajhne mein mujhe thoda samay lag gaya.

I'll be sharing a step-by-step guide very soon.
Main bahut jald ek step-by-step guide share karunga.

She's an influencer known for her fashion tips and lifestyle vlogs.
Woh ek influencer hai jinko fashion tips aur lifestyle vlogs ke liye jaana jaata hai.

My video production setup includes a high-quality camera and professional lighting.
Mere video production setup mein ek high-quality camera aur professional lighting setup liya hai.

Collaborating with fellow creators can lead to fresh and exciting content ideas.
Doosre creators ke saath collaboration karke naye aur exciting content ideas mil sakti hai.

I use social media analytics to track the performance of my posts.
Main social media analytics ka use apne posts ke performance ko track karne ke liye karta hoon.

The key to building a loyal audience is consistency in content delivery.
Ek loyal audience banane ka mool mantra hai content delivery mein consistency.

I'm scripting my next video on tips for better storytelling.
Main apne agle video ke script par kaam kar raha hoon jo better storytelling ke tips par hoga.

Can you help me brainstorm ideas for my podcast episodes? The thumbnail for my latest video is eye-catching, don't you think? I've scheduled a live Q&A session with my subscribers this weekend.
Kya aap meri podcast episodes ke liye ideas brainstorm karne mein meri madad kar sakte hain? Meri latest video ka thumbnail eye-catching hai, kya aapko nahi lagta? Mainne is weekend par apne subscribers ke saath live Q&A session ka schedule kiya hai.

Let's bring everything together, and look at how you will use the reward model in the reinforcement learning process to update the LLM weights, and produce a human aligned model.
Chaliye sab kuch ek saath lete hain, aur dekhte hain ki aap kaise reward model ka upyog karke reinforcement learning process mein LLM ke weights ko update karenge, aur ek human aligned model utpann karenge.

Remember, you wend to start with a model that already has good performance on your task of interests.
Yaad rakhiye, aapko apne task ke liye pehle se hi acchi performance wala model se shuru karna hai.

First, you'll pass a prompt from your prompt dataset.
Pehle, aap apne prompt dataset se ek prompt ko pass karenge.

In this case, a dog is, to the instruct LLM, which then generates a completion, in this case a furry animal.
Is case mein, ek kutta hai, LLM ko instruct karenge, jo phir ek puri completion generate karta hai, is case mein ek furry animal.

 Next, you sent this completion, and the original prompt to the reward model as the prompt completion pair.
Agla, aap yeh completion aur asli prompt ko reward model ko bhejenge jaise prompt completion pair.

The reward model evaluates the pair based on the human feedback it was trained on, and returns a reward value.
Reward model pair ko evaluate karta hai insani feedback ke adhar par jis par usne training liya tha, aur ek reward value lautaata hai.

A higher value such as 0.24 as shown here represents a more aligned response.
Ek jyada value jaise 0.24 jo yahaan dikhaya gaya hai, ek zyada aligned response ko darshata hai.

A less aligned response would receive a lower value, such as negative 0.53.
Kam aligned response ko ek kam value jaise ki negative 0.53 milta hai.

You'll then pass this reward value for the prom completion pair to the reinforcement learning algorithm to update the weights of the LLM, and move it towards generating more aligned, higher reward responses.
Fir aap yeh reward value ko prompt completion pair ke liye reinforcement learning algorithm ko bhejenge LLM ke weights ko update karne ke liye, aur use generate karne ke liye zyada aligned, zyada reward responses.

 Let's call this intermediate version of the model the RL updated LLM.
Chaliye is beech ka version ko RL updated LLM bula lete hain.

These series of steps together forms a single iteration of the RLHF process.
Ye series of steps ek RLHF process ka single iteration banati hain.

These iterations continue for a given number of epics, similar to other types of fine tuning.
Ye iterations ek diye gaye number of epochs ke liye continue hoti hain, doosre fine-tuning ke prakar ke tarah.

Here you can see that the completion generated by the RL updated LLM receives a higher reward score, indicating that the updates to weights have resulted in a more aligned completion.
Yahaan dekha ja sakta hai ki RL updated LLM dwara generate ki gayi completion ko ek zyada reward score mila hai, iska matlab hai ki weights ke updates se ek zyada aligned completion aayi hai.

If the process is working well, you'll see the reward improving after each iteration as the model produces text that is increasingly aligned with human preferences.
Agar process acche se kaam kar rahi hai, toh aap har iteration ke baad reward mein sudhar dekhenge jab model text utpann karta hai jo insani pasand ke saath badhta hai.

You will continue this iterative process until your model is aligned based on some evaluation criteria.
Aap is iterative process ko aage badhate rahenge jab tak aapka model kisi moolyaankan criteria ke adhar par aligned ho jaata hai.

For example, reaching a threshold value for the helpfulness you defined.
For example, aapne helpfulness ke liye ek drekht value ko paane tak.

You can also define a maximum number of steps, for example, 20,000 as the stopping criteria.
Aap ek adhikansh sankhya ke kadam ka niyamit kar sakte hain, jaise ki 20,000, jaise ki rukavat ke maamle mein.

At this point, let's refer to the fine-tuned model as the human-aligned LLM.
Is samay, chaliye fine-tuned model ko human-aligned LLM ke roop mein refer karein.

One detail we haven't discussed yet is the exact nature of the reinforcement learning algorithm.
Ek cheez humne abhi tak nahi charcha ki hai, woh hai reinforcement learning algorithm ka vyakti swaroop.

This is the algorithm that takes the output of the reward model and uses it to update the LLM model weights so that the reward score increases over time.
Ye algorithm reward model ka output leta hai aur iska use LLM ke weights ko update karne mein karta hai taaki reward score samay ke saath badhe.

There are several different algorithms that you can use for this part of the RLHF process.
Is RLHF process ka is hisse ke liye aap kai alag algorithms ka use kar sakte hain.

 A popular choice is proximal policy optimization or PPO for short.
Ek lokpriya chunav hai proximal policy optimization ya PPO ke liye.

PPO is a pretty complicated algorithm, and you don't have to be familiar with all of the details to be able to make use of it.
PPO ek kaafi complicated algorithm hai, aur aapko iske sabhi tafseelat se parichit hone ki zaroorat nahi hai ise use karne ke liye.

However, it can be a tricky algorithm to implement and understanding its inner workings in more detail can help you troubleshoot if you're having problems getting it to work.
Lekin, agar aapko iskaam ko karne mein koi samasya aa rahi hai, toh iske andar kaam kaise karta hai ko samajhna aapki madad kar sakta hai.

To explain how the PPO algorithm works in more detail, I invited my AWS colleague, Ek to give you a deeper dive on the technical details.
PPO algorithm kaam kaise karta hai ko aur bhi vistaar se samjhane ke liye, maine apne AWS saathi, Ek ko bulaya hai aapko technical tafseelat mein gahraee se samjhane ke liye.

This next video is optional and you should feel free to skip it, and move on to the reward hacking video.
Ye agla video optional hai aur aap isko chhodkar seedhe reward hacking video par jaa sakte hain.

You won't need the information here to complete the quizzes or this week's lab.
Aapko quizzes ya is haft ke lab ko poora karne ke liye yahaan di gayi jaankari ki zaroorat nahi hogi.

However, I encourage you to check out the details as RLHF is becoming increasingly importend to ensure that LLMs behave in a safe and aligned manner in deployment.
Lekin, main aapko protsahan deta hoon ki aap tafseelat ko dekhein kyunki RLHF ki bhoomika badh rahi hai taaki LLMs deployment mein ek surakshit aur aligned tareeke se vyavhaar karein.

Using trl you can run one of the most popular Deep RL algorithms, PPO, in a distributed manner or on a single device! We leverage accelerate from the Hugging Face ecosystem to make this possible, so that any user can scale up the experiments up to an interesting scale.
trl ka use karke aap ek prasiddh Deep RL algorithms, PPO ko ek vitarit tareeke se ya ek single device par chala sakte hain! Hum Hugging Face ecosystem se accelerate ka use karte hain taaki koi bhi upyogkarta eksperiments ko ek rochak paimane tak scale kar sakein.

Fine-tuning a language model with RL follows roughly the protocol detailed below.
Ek language model ko RL ke saath fine-tune karna lagbhag neeche vistarit protocol ko follow karna shuru karta hai.

This requires having 2 copies of the original model; to avoid the active model deviating too much from its original behavior, distribution you need to compute the logits of the reference model at each optimization step.
Iska matlab hai ki aapke paas asli model ke 2 nukkaron ki kopiya honi chahiye; asli vyavhaar se bahut zyada bhatakne se bachne ke liye, aapko har optimization kadam par reference model ke logits ko compute karna hoga.

This adds a hard constraint on the optimization process as you need always at least two copies of the model per GPU device.
Ye ek kathin constraint ko optimization process par dalta hai kyun ki hamesha kam se kam do model ki kopiya ek GPU device ke liye chahiye hoti hai.

If the model grows in size, it becomes more and more tricky to fit the setup on a single GPU.
Agar model ka size badh jaata hai, toh ek GPU par setup fit karna aur bhi zyada mushkil ho jaata hai.

Low-rank Adaptation, or LoRA for short, is a parameter-efficient fine-tuning technique that falls into the re-parameterization category.
Low-rank Adaptation, ya LoRA in short, ek parameter-efficient fine-tuning technique hai jo re-parameterization category mein aata hai.

Let's take a look at how it works.
Chaliye dekhte hain ki ye kaise kaam karta hai.

As a quick reminder, here's the diagram of the transformer architecture that you saw earlier in the course.
Ek tezi se yaad dilane ke liye, yahaan pehle se padha hua transformer architecture ka diagram hai jo aapne is course mein pehle dekha tha.

The input prompt is turned into tokens, which are then converted to embedding vectors and passed into the encoder and/or decoder parts of the transformer.
Input prompt ko tokens mein badal diya jaata hai, jo phir embedding vectors mein convert kiye jaate hain aur transformer ke encoder aur/va decoder hisson mein pass kiye jaate hain.

In both of these components, there are two kinds of neural networks; self-attention and feedforward networks.
In dono components mein, do prakar ke neural networks hote hain; self-attention aur feedforward networks.

The weights of these networks are learned during pre-training.
In networks ke weights pre-training ke dauran sikhe jaate hain.

After the embedding vectors are created, they're fed into the self-attention layers where a series of weights are applied to calculate the attention scores.
Embedding vectors banane ke baad, unhe self-attention layers mein feed kiya jaata hai jahan ek series of weights ka use attention scores ka calculation karne ke liye kiya jaata hai.

During full fine-tuning, every parameter in these layers is updated.
Poora fine-tuning ke dauran, in layers ke har parameter ko update kiya jaata hai.

LoRA is a strategy that reduces the number of parameters to be trained during fine-tuning by freezing all of the original model parameters and then injecting a pair of rank decomposition matrices alongside the original weights.
LoRA ek strategy hai jo fine-tuning ke dauran train ki jaane waali parameters ki sankhya ko kam kar deta hai jo ki original model ke saath freeze kiye gaye hote hain aur phir original weights ke saath ek pair of rank decomposition matrices ko inject karta hai.

The dimensions of the smaller matrices are set so that their product is a matrix with the same dimensions as the weights they're modifying.
Chhote matrices ke dimensions itne set kiye jaate hain ki unka product unke modify kiye gaye weights ke samaan dimensions wala ek matrix ho.

You then keep the original weights of the LLM frozen and train the smaller matrices using the same supervised learning process you saw earlier this week.
Fir aap LLM ke original weights ko freeze karte hain aur in chhoti matrices ko wahi supervised learning process se train karte hain jo aapne is hafte pehle dekha tha.

For inference, the two low-rank matrices are multiplied together to create a matrix with the same dimensions as the frozen weights.
Inference ke liye, do low-rank matrices ko ek dusre se multiply kiya jaata hai ek matrix utpann karne ke liye jo freeze ki gayi weights ke samaan dimensions wala hota hai.

You then add this to the original weights and replace them in the model with these updated values.
Fir aap isko original weights mein jodte hain aur model mein in naye updated values ko daalte hain.

You now have a LoRA fine-tuned model that can carry out your specific task.
Ab aapke paas ek LoRA fine-tuned model hai jo aapki khaas task ko kar sakta hai.

Because this model has the same number of parameters as the original, there is little to no impact on inference latency.
Kyunki is model mein original ke samaan sankhya ke parameters hote hain, iska inference latency par koi asar nahi hota hai.

Researchers have found that applying LoRA to just the self-attention layers of the model is often enough to fine-tune for a task and achieve performance gains.
Researchers ne paya hai ki LoRA ko model ke sirf self-attention layers par lagane se aksar task ke liye fine-tune kar sakte hain aur performance ko badha sakte hain.

However, in principle, you can also use LoRA on other components like the feed-forward layers.
Lekin, tatva mein, aap feed-forward layers jaise doosre components par bhi LoRA ka use kar sakte hain.

But since most of the parameters of LLMs are in the attention layers, you get the biggest savings in trainable parameters by applying LoRA to these weights matrices.
Lekin kyunki LLMs ke adhikansh parameters self-attention layers mein hote hain, aap in weights matrices par LoRA ka use karke train karne se trainable parameters mein sabse badi bachat pa sakte hain.

Let's look at a practical example using the transformer architecture described in the Attention is All You Need paper.
Chaliye ek vyavharik udaharan dekhte hain jo Attention is All You Need paper mein di gayi transformer architecture ka use karta hai.

The paper specifies that the transformer weights have dimensions of 512 by 64.
Paper deta hai ki transformer ke weights ke dimensions 512 by 64 hote hain.

This means that each weights matrix has 32,768 trainable parameters.
Iska matlab hai ki har weights matrix mein 32,768 trainable parameters hote hain.

If you use LoRA as a fine-tuning method with the rank equal to eight, you will instead train two small rank decomposition matrices whose small dimension is eight.
Agar aap LoRA ko ek fine-tuning method ke roop mein rank barabar aath ke saath use karte hain, toh aap do chhote rank decomposition matrices ko train karenge jinki chhoti dimension aath hoti hai.

This means that Matrix A will have dimensions of 8 by 64, resulting in 512 total parameters.
Iska matlab hai ki Matrix A ke dimensions 8 by 64 honge, jisme 512 total parameters honge.

Matrix B will have dimensions of 512 by 8, or 4,096 trainable parameters.
Matrix B ke dimensions 512 by 8 honge, yaani 4,096 trainable parameters honge.

By updating the weights of these new low-rank matrices instead of the original weights, you'll be training 4,608 parameters instead of 32,768 and 86% reduction.
In naye low-rank matrices ke weights ko update karke original weights ke bajaye, aap 32,768 ke bajaye 4,608 parameters ko train karenge aur 86% reduction hoga.

Because LoRA allows you to significendly reduce the number of trainable parameters, you can often perform this method of parameter efficient fine tuning with a single GPU and avoid the need for a distributed cluster of GPUs.
Kyunki LoRA aapko trainable parameters ki sankhya ko significendly kam karne ki anumati deta hai, aap is method ko ek single GPU ke saath karte hue aksar kar sakte hain aur ek distributed cluster of GPUs ki zaroorat se bach sakte hain.

Since the rank-decomposition matrices are small, you can fine-tune a different set for each task and then switch them out at inference time by updating the weights.
Kyunki rank-decomposition matrices chhote hote hain, aap alag alag task ke liye ek naye set ke liye fine-tune kar sakte hain aur phir inference ke samay inhein badal sakte hain weights ko update karke.

Suppose you train a pair of LoRA matrices for a specific task; let's call it Task A.
Maan lijiye aapne kisi vishesh task ke liye ek pair of LoRA matrices ko train kiya; chaliye ise Task A bulate hain.

To carry out inference on this task, you would multiply these matrices together and then add the resulting matrix to the original frozen weights.
Is task par inference ko carry out karne ke liye, aap in matrices ko multiply karenge aur phir is matrix ko original frozen weights mein jodenge.

You then take this new summed weights matrix and replace the original weights where they appear in your model.
Fir aap is naye sums weights matrix ko original weights mein badalenge jahan yeh aapke model mein dikhai dete hain.

You can then use this model to carry out inference on Task A.
Ab aap is model ka use Task A par inference karne ke liye kar sakte hain.

If instead, you wend to carry out a different task, say Task B, you simply take the LoRA matrices you trained for this task, calculate their product, and then add this matrix to the original weights and update the model again.
Agar aapko thayd mein, koi alag task, maan lijiye Task B, carry out karna hai, toh aap bas Task ke liye train kiye gaye LoRA matrices ko lenge, unka product calculate karenge, aur phir is matrix ko original weights mein add karenge aur phir se model ko update karenge.

The memory required to store these LoRA matrices is very small.
In LoRA matrices ko store karne ke liye zaroori memory bahut kam hoti hai.

So in principle, you can use LoRA to train for many tasks.
Iska matlab hai ki tatva mein, aap LoRA ka use karke bahut saare tasks ke liye train kar sakte hain.

Switch out the weights when you need to use them, and avoid having to store multiple full-size versions of the LLM.
Jab aap unhe use karne ki zaroorat ho, toh unhe weights ko badalne se, aur LLM ke multiple full-size versions ko store karne ki zaroorat se bach sakte hain.

How good are these models? This data is pulled from many sources, including scrapes off the Internet and corpora of texts that have been assembled specifically for training language models.
Ye models kitne acche hain? Ye data kayi sources se liya gaya hai, jismein internet se scrape ki gayi jaankariyon aur text corpora shaamil hain jo language models ko training ke liye specifically compile kiye gaye hain.

In this self-supervised learning step, the model internalizes the patterns and structures present in the language.
Is self-supervised learning ke kadam mein, model bhasha mein mojood patterns aur structures ko internalize karta hai.

These patterns then enable the model to complete its training objective, which depends on the architecture of the model, as you'll see shortly.
Ye patterns phir model ko uske training objective ko poora karne mein madad karte hain, jo model ke architecture par nirbhar karta hai, jaise ki aap jald hi dekhenge.

During pre-training, the model weights get updated to minimize the loss of the training objective.
Pre-training ke dauran, model weights ko update kiya jaata hai training objective ke loss ko minimize karne ke liye.

The encoder generates an embedding or vector representation for each token.
Encoder har token ke liye ek embedding ya vector representation banata hai.

Pre-training also requires a large amount of compute and the use of GPUs.
Pre-training ko bhi ek bada amount of compute aur GPUs ka upyog karna hota hai.

Note, when you scrape training data from public sites such as the Internet, you often need to process the data to increase quality, address bias, and remove other harmful content.
Note karein, jab aap public sites jaise ki internet se training data scrape karte hain, toh aapko data ko process karne ki zaroorat hoti hai quality ko badhane ke liye, bias ko address karne ke liye, aur doosre harmful content ko remove karne ke liye.

As a result of this data quality curation, often only 1-3% of tokens are used for pre-training.
Is data quality curation ke natije mein, aksar sirf 1-3% tokens ka use pre-training ke liye hota hai.

You should consider this when you estimate how much data you need to collect if you decide to pre-train your own model.
Aapko yeh dhyan mein rakhna chahiye jab aap apna khud ka model pre-train karne ka faisla karte hain.

Earlier this week, you saw that there were three variance of the transformer model; encoder-only encoder-decoder models, and decode-only.
Is hafte pehle, aapne dekha ki transformer model ke teen prakar hain; sirf encoder, encoder-decoder models, aur decode-only.

Each of these is trained on a different objective, and so learns how to carry out different tasks.
Har ek ka alag objective hai, aur isliye har ek alag task ko karne ka tarika seekhta hai.

Encoder-only models are also known as Autoencoding models, and they are pre-trained using masked language modeling.
Sirf encoder wale models ko Autoencoding models bhi jaante hain, aur ye masked language modeling ka use karke pre-train hote hain.

Here, tokens in the input sequence or randomly mask, and the training objective is to predict the mask tokens in order to reconstruct the original sentence.
Yahaan, input sequence ke tokens ko ya toh randomly mask kiya jata hai, aur training objective mask tokens ko predict karne ka hota hai taki asli sentence ko reconstruct kiya ja sake.

This is also called a denoising objective.
Ye bhi ek denoising objective ke roop mein jaana jaata hai.

Autoencoding models spilled bi-directional representations of the input sequence, meaning that the model has an understanding of the full context of a token and not just of the words that come before.
Autoencoding models input sequence ka bi-directional representations banaate hain, matlab ki model ko ek token ka poora context samajh aata hai aur sirf pehle aane waale shabdon ka nahi.

Encoder-only models are ideally suited to task that benefit from this bi-directional contexts.
Sirf encoder wale models ideally suited hote hain task ke liye jo is bi-directional context se faayda uthate hain.

You can use them to carry out sentence classification tasks, for example, sentiment analysis or token-level tasks like named entity recogpolicyon or word classification.
Aap inhe sentence classification tasks ke liye use kar sakte hain, jaise ki sentiment analysis ya token-level tasks jaise named entity recognition ya word classification.

Some well-known examples of an autoencoder model are BERT and RoBERTa.
Kuch prasiddh udaharan ek autoencoder model ke hain BERT aur RoBERTa.

Now, let's take a look at decoder-only or autoregressive models, which are pre-trained using causal language modeling.
Ab, chaliye dekhte hain decoder-only ya autoregressive models ko, jo causal language modeling ka use karke pre-train hote hain.

Here, the training objective is to predict the next token based on the previous sequence of tokens.
Yahaan, training objective hai ki agle token ka prediction karna pehle ke tokens ke sequence ke adhar par.

Predicting the next token is sometimes called full language modeling by researchers.
Agla token predict karna kabhi kabhi researchers dwaara full language modeling kehte hain.

Decoder-based autoregressive models, mask the input sequence and can only see the input tokens leading up to the token in question.
Decoder-based autoregressive models, input sequence ko mask karte hain aur sirf input tokens ko dekhte hain jo sawal mein hain.

The model has no knowledge of the end of the sentence.
Model ko vaakya ke ant ke baare mein koi gyaan nahi hota.

The model then iterates over the input sequence one by one to predict the following token.
Model fir input sequence ke har ek token ko predict karne ke liye ek ek karke iterate karta hai.

A useful example to illustrate these ideas is training a model to play Tic-Tac-Toe.
Ye ideas ko illustrate karne ke liye ek upyogi udaharan hai model ko Tic-Tac-Toe khelne ke liye train karna.

Let's take a look.
Chaliye dekhte hain.

In this example, the agent is a model or policy acting as a Tic-Tac-Toe player.
Is udaharan mein, agent ek model ya policy hai jo Tic-Tac-Toe player ke roop mein kaam karta hai.

Its objective is to win the game.
Iska uddeshya game jeetna hota hai.

The environment is the three by three game board, and the state at any moment, is the current configuration of the board.
Environment teen by teen game board hota hai, aur kisi bhi samay ka state, board ka current configuration hota hai.

The action space comprises all the possible positions a player can choose based on the current board state.
Action space mein player jo bhi possible positions choose kar sakta hai, uska vyavhaar sthiti ke adhar par hota hai.

The agent makes decisions by following a strategy known as the RL policy.
Agent RL policy ke naam se jaani jaati hai.

Now, as the agent takes actions, it collects rewards based on the actions' effectiveness in progressing towards a win.
Ab, jab agent actions leta hai, toh woh actions ki effectiveness ke adhar par rewards collect karta hai jo game jeetne ki taraf pragati ke liye hai.

The goal of reinforcement learning is for the agent to learn the optimal policy for a given environment that maximizes their rewards.
Reinforcement learning ka uddeshya hai ki agent ko ek diye gaye environment ke liye ek optimal policy sikhna hai jo unke rewards ko maximum banaata hai.

This learning process is iterative and involves trial and error.
Ye learning process iterative hota hai aur trial and error ke saath hota hai.

Ipolicyally, the agent takes a random action which leads to a new state.
Aam taur par, agent ek random action leta hai jo ek naya state tak le jaata hai.

From this state, the agent proceeds to explore subsequent states through further actions.
Is state se, agent agle actions ke zariye aane waale states ko explore karne ke liye aage badhta hai.

The series of actions and corresponding states form a playout, often called a rollout.
Actions aur corresponding states ka silsila ek playout ya rollout banate hain, jise aksar rollout kaha jata hai.

As the agent accumulates experience, it gradually uncovers actions that yield the highest long-term rewards, ultimately leading to success in the game.
Jab agent anubhav jama karta hai, toh woh dheere dheere un actions ko khole jata hai jo sabse zyada lambi dhaar rewards dete hain, ant mein khel mein safalta tak pahunchate hue.

Now let's take a look at how the Tic-Tac-Toe example can be extended to the case of fine-tuning large language models with RLHF.
Ab chaliye dekhte hain ki Tic-Tac-Toe udaharan ko kaise fine-tune kar sakte hain RLHF ke saath.

In this case, the agent's policy that guides the actions is the LLM, and its objective is to generate text that is perceived as being aligned with the human preferences.
Is case mein, agent ka policy jo actions ko guide karta hai woh LLM hai, aur uska uddeshya hai text generate karna jo insani preferences ke saath aligned hai.